c(4,.8), rot.per=0, random.order=F)
for(i in 1:30){
topic.top.words <- mallet.top.words(topic.model,
topic.words.m[i,], 100)
print(wordcloud(topic.top.words$words,
topic.top.words$weights,
c(4,.8), rot.per=0,
random.order=F))
}
for(i in 1:30){
topic.top.words <- mallet.top.words(topic.model,
topic.words[i,], 100)
print(wordcloud(topic.top.words$words,
topic.top.words$weights,
c(4,.8), rot.per=0,
random.order=F))
}
for(i in 1:30){
topic.top.words <- mallet.top.words(topic.model,
topic.words[i,], 25)
print(wordcloud(topic.top.words$words,
topic.top.words$weights,
c(4,.8), rot.per=0,
random.order=F))
}
for(i in 1:30){
topic.top.words <- mallet.top.words(topic.model,
topic.words[i,], 25)
print(wordcloud(topic.top.words$words,
topic.top.words$weights,
c(4,.8), rot.per=0,
random.order=T))
}
library(cluster)
topic_df_dist <- as.matrix(daisy(t(topic_docs), metric = "euclidean", stand = TRUE))
topic_df_dist <- as.matrix(daisy(t(topic.docs), metric = "euclidean", stand = TRUE))
topic_df_dist[ sweep(topic_df_dist, 1, (apply(topic_df_dist,1,min) + apply(topic_df_dist,1,sd) )) > 0 ] <- 0
km <- kmeans(topic_df_dist, n.topics)
allnames <- vector("list", length = n.topics)
for(i in 1:n.topics){
allnames[[i]] <- names(km$cluster[km$cluster == i])
}
km <- kmeans(topic_df_dist, n.topics)
library(igraph)
install.packages("igraph")
g <- as.undirected(graph.adjacency(topic_df_dist))
layout1 <- layout.fruchterman.reingold(g, niter=500)
plot(g, layout=layout1, edge.curved = TRUE, vertex.size = 1, vertex.color= "grey", edge.arrow.size = 0, vertex.label.dist=0.5, vertex.label = NA)
g <- as.undirected(graph.adjacency(topic_df_dist))
head(km)
options(java.parameters = "-Xmx5120m")
library(rJava)
setwd("desktop/data mining and tools/datasets/greek-vases-justin-walsh/vases-table/")
library(mallet)
documents <- read.table("modified-vases3.txt", col.names=c("id", "class", "text"),
colClasses=rep("character", 3), sep="\t", quote="")
mallet.instances <- mallet.import(documents$id, documents$text, "/Users/shawngraham/Desktop/data mining and tools/TextAnalysisWithR/data/stoplist.csv",
token.regexp = "\\p{L}[\\p{L}\\p{P}]+\\p{L}")
num.topics <- 20
topic.model <- MalletLDA(num.topics)
topic.model$loadDocuments(mallet.instances)
vocabulary <- topic.model$getVocabulary()
word.freqs <- mallet.word.freqs(topic.model)
head(word.freqs)
topic.model$setAlphaOptimization(20, 50)
topic.model$train(1000)
topic.model$maximize(10)
doc.topics <- mallet.doc.topics(topic.model, smoothed=T, normalized=T)
topic.words <- mallet.topic.words(topic.model, smoothed=T, normalized=T)
indig.topic.words <- mallet.subset.topic.words(topic.model, documents$class == "TRUE",
smoothed=T, normalized=T)
notindig.topic.words <- mallet.subset.topic.words(topic.model, documents$class == "FALSE",
smoothed=T, normalized=T)
mallet.top.words(topic.model, indig.topic.words[10,])
mallet.top.words(topic.model, notindig.topic.words[10,])
topic.docs <- t(doc.topics)
topic.docs <- topic.docs / rowSums(topic.docs)
topics.labels <- rep("", num.topics)
for (topic in 1:num.topics) topics.labels[topic] <- paste(mallet.top.words(topic.model, topic.words[topic,], num.top.words=5)$words, collapse=" ")
topics.labels
topic_docs <- data.frame(topic.docs)
names(topic_docs) <- documents$id
library(cluster)
topic_df_dist <- as.matrix(daisy(t(topic_docs), metric = "euclidean", stand = TRUE))
dim(topic_df_dist)
documents <- mallet.read.dir("mallet-2.0.7/sample-data/web/en/")
mallet.instances <- mallet.import(documents$id, documents$text, "mallet-2.0.7/stoplists/en.txt", token.regexp = "\\p{L}[\\p{L}\\p{P}]+\\p{L}")
n.topics <- 30
topic.model <- MalletLDA(n.topics)
topic.model$loadDocuments(mallet.instances)
vocabulary <- topic.model$getVocabulary()
word.freqs <- mallet.word.freqs(topic.model)
topic.model$setAlphaOptimization(20, 50)
topic.model$train(200)
topic.model$maximize(10)
doc.topics <- mallet.doc.topics(topic.model, smoothed=T, normalized=T)
topic.words <- mallet.topic.words(topic.model, smoothed=T, normalized=T)  ##adap jockers wordcloud script to use this variable
topic.docs <- t(doc.topics)
topic.docs <- topic.docs / rowSums(topic.docs)
topics.labels <- rep("", n.topics)
for (topic in 1:n.topics) topics.labels[topic] <- paste(mallet.top.words(topic.model, topic.words[topic,], num.top.words=5)$words, collapse=" ")
topics.labels
documents <- mallet.read.dir("mallet-2.0.7/sample-data/web/en/")
documents <- mallet.read.dir("shawngraham/mallet-2.0.7/sample-data/web/en/")
setwd('shawngraham')
getwd()
setwd('/Users/shawngraham/')
documents <- mallet.read.dir("shawngraham/mallet-2.0.7/sample-data/web/en/")
mallet.instances <- mallet.import(documents$id, documents$text, "mallet-2.0.7/stoplists/en.txt", token.regexp = "\\p{L}[\\p{L}\\p{P}]+\\p{L}")
documents <- mallet.read.dir("shawngraham/mallet-2.0.7/sample-data/web/en/")
documents <- mallet.read.dir("/mallet-2.0.7/sample-data/web/en/")
documents <- mallet.read.dir("mallet-2.0.7/sample-data/web/en/")
mallet.instances <- mallet.import(documents$id, documents$text, "mallet-2.0.7/stoplists/en.txt", token.regexp = "\\p{L}[\\p{L}\\p{P}]+\\p{L}")
n.topics <- 30
topic.model <- MalletLDA(n.topics)
topic.model$loadDocuments(mallet.instances)
vocabulary <- topic.model$getVocabulary()
word.freqs <- mallet.word.freqs(topic.model)
topic.model$setAlphaOptimization(20, 50)
topic.model$train(200)
topic.model$maximize(10)
doc.topics <- mallet.doc.topics(topic.model, smoothed=T, normalized=T)
topic.words <- mallet.topic.words(topic.model, smoothed=T, normalized=T)  ##adap jockers wordcloud script to use this variable
topic.docs <- t(doc.topics)
topic.docs <- topic.docs / rowSums(topic.docs)
topics.labels <- rep("", n.topics)
for (topic in 1:n.topics) topics.labels[topic] <- paste(mallet.top.words(topic.model, topic.words[topic,], num.top.words=5)$words, collapse=" ")
topics.labels
library(cluster)
topic_df_dist <- as.matrix(daisy(t(topic_docs), metric = "euclidean", stand = TRUE))
topic_docs <- data.frame(topic.docs)
names(topic_docs) <- documents$id
topic_df_dist <- as.matrix(daisy(t(topic_docs), metric = "euclidean", stand = TRUE))
topic_df_dist[ sweep(topic_df_dist, 1, (apply(topic_df_dist,1,min) + apply(topic_df_dist,1,sd) )) > 0 ] <- 0
km <- kmeans(topic_df_dist, num.topics)
km <- kmeans(topic_df_dist, n.topics)
km <- kmeans(topic_df_dist, 30)
km <- kmeans(topic_df_dist, 20)
n.topics
km <- kmeans(topic_df_dist, n.topics)
km <- kmeans(topic_df_dist, 2)
km <- kmeans(topic_df_dist, 20)
km <- kmeans(topic_df_dist, 10)
km <- kmeans(topic_df_dist, 5)
km <- kmeans(topic_df_dist, 8)
km <- kmeans(topic_df_dist, 9)
km <- kmeans(topic_df_dist, 10)
km <- kmeans(topic_df_dist, 15)
km <- kmeans(topic_df_dist, 12)
k
m
km <- kmeans(topic_df_dist, 13)
km <- kmeans(topic_df_dist, 12)
library(devtools)
create("-/shawngraham/githubstuff/scriv-opennotes/scriv-opennotes/another-project/Rdataanalysis")
create("~/shawngraham/githubstuff/scriv-opennotes/scriv-opennotes/another-project/Rdataanalysis")
create("~/githubstuff/scriv-opennotes/scriv-opennotes/another-project/Rdataanalysis")
install.packages("roxygen2")
library(devtools)
create("~/githubstuff/scriv-opennotes/scriv-opennotes/another-project/Rdataanalysis")
require(mallet)
documents <- mallet.read.dir("mallet-2.0.7/sample-data/web/en/")
mallet.instances <- mallet.import(documents$id, documents$text, "mallet-2.0.7/stoplists/en.txt", token.regexp = "\\p{L}[\\p{L}\\p{P}]+\\p{L}")
#create topic trainer object
n.topics <- 30
topic.model <- MalletLDA(n.topics)
#load documents
topic.model$loadDocuments(mallet.instances)
require(mallet)
documents <- mallet.read.dir("mallet-2.0.7/sample-data/web/en/")
mallet.instances <- mallet.import(documents$id, documents$text, "mallet-2.0.7/stoplists/en.txt", token.regexp = "\\p{L}[\\p{L}\\p{P}]+\\p{L}")
n.topics <- 30
topic.model <- MalletLDA(n.topics)
topic.model$loadDocuments(mallet.instances)
library(knitr)
library(markdown)
getwd()
library(wordcloud)
for(i in 1:30){
topic.top.words <- mallet.top.words(topic.model,
topic.words[i,], 10)
print(wordcloud(topic.top.words$words,
topic.top.words$weights,
c(4,.8), rot.per=0,
random.order=F))
}
topic.model <- MalletLDA(n.topics)
#' ---
#' title: "An Example Topic Model Script"
#' author: "Shawn Graham"
#' date: "Sept 29 2014"
#' ---
#'
#' This is for an example of using Mimno's wrapper for Mallet
#' we're using the sample data that comes bundeled when you download MALLET from
#' http://mallet.cs.umass.edu/download.php
#' we've assumed that, on Mac, you've put MALLET that you unzipped from Umass into a folder
#' under [user], ie "shawngraham/mallet-2.0.7"
#' on windows, use the full path "C:>\\mallet-2.0.7\\"
#' insert the path to your documents between the quotation marks
#' and windows users be sure to use \\ instead of a single \
#' we're assuming that you've already installed the mallet wrapper for R; if not, uncomment and run this line:
#' install.packages('mallet')
#' if you are using Mavericks OS there could be a problem in installation - see chapter four for solution.
require(mallet)
#' import the documents from the folder
#' each document is here its own text file
documents <- mallet.read.dir("mallet-2.0.7/sample-data/web/en/")
---
---
This is an example of using Mimno's wrapper for Mallet. We're using the sample data that comes bundeled when you download MALLET from http://mallet.cs.umass.edu/download.php. We've assumed that, on Mac, you've put MALLET that you unzipped from Umass into a folder under [user], ie "shawngraham/mallet-2.0.7". On windows, use the full path "C:>\\mallet-2.0.7\\"
---
title: "An Example Topic Model Script"
author: "Shawn Graham"
date: "Sept 29 2014"
output: md_document
---
This is an example of using Mimno's wrapper for Mallet. We're using the sample data that comes bundeled when you download MALLET from http://mallet.cs.umass.edu/download.php. We've assumed that, on Mac, you've put MALLET that you unzipped from Umass into a folder under [user], ie "shawngraham/mallet-2.0.7". On windows, use the full path "C:>\\mallet-2.0.7\\"
Insert the path to your documents between the quotation marks and windows users be sure to use \\ instead of a single \
We're assuming that you've already installed the mallet wrapper for R; if not, uncomment and run this line:
```
install.packages('mallet')
require(mallet)
```
- import the documents from the folder
- each document is here its own text file
```
documents <- mallet.read.dir("mallet-2.0.7/sample-data/web/en/")
mallet.instances <- mallet.import(documents$id, documents$text, "mallet-2.0.7/stoplists/en.txt", token.regexp = "\\p{L}[\\p{L}\\p{P}]+\\p{L}")
```
- create topic trainer object
```n.topics <- 30
topic.model <- MalletLDA(n.topics)
```
- load documents
```topic.model$loadDocuments(mallet.instances)
```
- Get the vocabulary, and some statistics about word frequencies.
- These may be useful in further curating the stopword list.
```vocabulary <- topic.model$getVocabulary()
word.freqs <- mallet.word.freqs(topic.model)
```
- Optimize hyperparameters every 20 iterations, after 50 burn-in iterations.
```topic.model$setAlphaOptimization(20, 50)
```
- Now train a model. Note that hyperparameter optimization is on, by default.
- We can specify the number of iterations. Here we'll use a large-ish round number.
```topic.model$train(200)
```
- NEW: run through a few iterations where we pick the best topic for each token, rather than sampling from the posterior distribution.
```topic.model$maximize(10)
```
- Get the probability of topics in documents and the probability of words in topics.
- By default, these functions return raw word counts. Here we want probabilities,
so we normalize, and add "smoothing" so that nothing has exactly 0 probability.
```doc.topics <- mallet.doc.topics(topic.model, smoothed=T, normalized=T)
topic.words <- mallet.topic.words(topic.model, smoothed=T, normalized=T)  ```
- transpose and normalize the doc topics
```topic.docs <- t(doc.topics)
topic.docs <- topic.docs / rowSums(topic.docs)
write.csv(topic.docs, "topics-docs.csv" )
```
- Get a vector containing short names for the topics
```topics.labels <- rep("", n.topics)
for (topic in 1:n.topics) topics.labels[topic] <- paste(mallet.top.words(topic.model, topic.words[topic,], num.top.words=5)$words, collapse=" ")
```
- have a look at keywords for each topic
```topics.labels
write.csv(topics.labels, "topics-labels.csv") ## "C:\\Mallet-2.0.7\\topics-labels.csv")
```
- Now you can visualize the patterns. For instance we could do word clouds of the topics
```library(wordcloud)
for(i in 1:30){
topic.top.words <- mallet.top.words(topic.model,
topic.words[i,], 10)
print(wordcloud(topic.top.words$words,
topic.top.words$weights,
c(4,.8), rot.per=0,
random.order=F))
}
```
- The End
install.packages("mallet")
source('~/mimnowrapperexample-w-viz copy.R')
options(java.parameters="-Xmx3584m")
library(rJava)
library(mallet)
library(dfrtopics)
library(devtools)
install_github("dfrtopics","agoldst")
getwd()
setwd(~/desktop/data mining and tools)
setwd(~\desktop\data mining and tools)
setwd(~desktop/ata mining and tools)
setwd(~desktop/data mining and tools)
setwd(~\\desktop\\data mining and tools)
setwd(desktop\\data mining and tools)
setwd("~desktop\\data mining and tools")
setwd("esktop\\data mining and tools")
setwd("desktop/data mining and tools")
getwd()
require(mallet)
documents <- mallet.read.dir("FergusontText")
mallet.instances <- mallet.import(documents$id, documents$text, "mallet-2.0.7/stoplists/en.txt", token.regexp = "\\p{L}[\\p{L}\\p{P}]+\\p{L}")
mallet.instances <- mallet.import(documents$id, documents$text, "mallet/stoplists/en.txt", token.regexp = "\\p{L}[\\p{L}\\p{P}]+\\p{L}")
mallet.instances <- mallet.import(documents$id, documents$text, "en.txt", token.regexp = "\\p{L}[\\p{L}\\p{P}]+\\p{L}")
mallet.instances <- mallet.import(documents$id, documents$text, "en.txt", token.regexp = "\\p{L}[\\p{L}\\p{P}]+\\p{L}")
n.topics <- 100
topic.model <- MalletLDA(n.topics)
topic.model$loadDocuments(mallet.instances)
vocabulary <- topic.model$getVocabulary()
word.freqs <- mallet.word.freqs(topic.model)
topic.model$setAlphaOptimization(20, 50)
topic.model$train(200)
topic.model$maximize(10)
doc.topics <- mallet.doc.topics(topic.model, smoothed=T, normalized=T)
topic.words <- mallet.topic.words(topic.model, smoothed=T, normalized=T)  ##adap jockers wordcloud script to use this variable
topic.docs <- t(doc.topics)
topic.docs <- topic.docs / rowSums(topic.docs)
write.csv(topic.docs, "topics-docs.csv" ) ## "C:\\Mallet-2.0.7\\topic-docs.csv"
topics.labels <- rep("", n.topics)
for (topic in 1:n.topics) topics.labels[topic] <- paste(mallet.top.words(topic.model, topic.words[topic,], num.top.words=5)$words, collapse=" ")
topics.labels
write.csv(topics.labels, "topics-labels.csv") ## "C:\\Mallet-2.0.7\\topics-labels.csv")
library(wordcloud)
for(i in 1:10){
topic.top.words <- mallet.top.words(topic.model,
topic.words[i,], 10)
print(wordcloud(topic.top.words$words,
topic.top.words$weights,
c(4,.8), rot.per=0,
random.order=F))
}
library(cluster)
topic_df_dist <-  as.matrix(daisy(t(topic_docs), metric =  "euclidean", stand = TRUE))
topic_docs <- data.frame(topic.docs)
names(topic_docs) <- documents$id
df1 <- t(topic_docs[,grep("Sarah Bennett", names(topic_docs))])
colnames(df1) <- topics.labels
require(reshape2)
topic.proportions.df <- melt(cbind(data.frame(df1),
document=factor(1:nrow(df1))),
variable.name="topic",
id.vars = "document")
require(ggplot2)
ggplot(topic.proportions.df, aes(topic, value, fill=document)) +
geom_bar(stat="identity") +
ylab("proportion") +
theme(axis.text.x = element_text(angle=90, hjust=1)) +
coord_flip() +
facet_wrap(~ document, ncol=5)
library(cluster)
topic_df_dist <-  as.matrix(daisy(t(topic_docs), metric =  "euclidean", stand = TRUE))
topic_df_dist[ sweep(topic_df_dist, 1, (apply(topic_df_dist,1,min) + apply(topic_df_dist,1,sd) )) > 0 ] <- 0
plot(hclust(dist(topic.words)), labels=topics.labels)
km <- kmeans(topic_df_dist, n.topics)
allnames <- vector("list", length = n.topics)
for(i in 1:n.topics){
allnames[[i]] <- names(km$cluster[km$cluster == i])
}
km <- kmeans(topic_df_dist, n.topics)
# get names for each cluster
allnames <- vector("list", length = n.topics)
for(i in 1:n.topics){
allnames[[i]] <- names(km$cluster[km$cluster == i])
}
# Here's the list of authors by group
allnames
km <- kmeans(topic_df_dist, n.topics)
library(igraph)
g <- as.undirected(graph.adjacency(topic_df_dist))
layout1 <- layout.fruchterman.reingold(g, niter=500)
plot(g, layout=layout1, edge.curved = TRUE, vertex.size = 1,  vertex.color= "grey", edge.arrow.size = 0, vertex.label.dist=0.5, vertex.label = NA)
library(cluster)
topic_df_dist <-  as.matrix(daisy(t(topic_docs), metric =  "euclidean", stand = TRUE))
# Change row values to zero if less than row minimum plus row standard deviation
# keep only closely related documents and avoid a dense spagetti diagram
# that's difficult to interpret (hat-tip: http://stackoverflow.com/a/16047196/1036500)
topic_df_dist[ sweep(topic_df_dist, 1, (apply(topic_df_dist,1,min) + apply(topic_df_dist,1,sd) )) > 0 ] <- 0
# that's difficult to interpret (hat-tip: http://stackoverflow.com/a/16047196/1036500)
topic_df_dist[ sweep(topic_df_dist, 1, (apply(topic_df_dist,1,min) + apply(topic_df_dist,1,sd) )) > 0 ] <- 0
library(cluster)
topic_df_dist <-  as.matrix(daisy(t(topic_docs), metric =  "euclidean", stand = TRUE))
# Change row values to zero if less than row minimum plus row standard deviation
# keep only closely related documents and avoid a dense spagetti diagram
# that's difficult to interpret (hat-tip: http://stackoverflow.com/a/16047196/1036500)
topic_df_dist[ sweep(topic_df_dist, 1, (apply(topic_df_dist,1,min) + apply(topic_df_dist,1,sd) )) > 0 ] <- 0
print(topic_df_dist)
library(cluster)
topic_df_dist <-  as.matrix(daisy(t(topic_docs), metric =  "euclidean", stand = TRUE))
# Change row values to zero if less than row minimum plus row standard deviation
# keep only closely related documents and avoid a dense spagetti diagram
# that's difficult to interpret (hat-tip: http://stackoverflow.com/a/16047196/1036500)
topic_df_dist[ sweep(topic_df_dist, 1, (apply(topic_df_dist,1,min) + apply(topic_df_dist,1,sd) )) > 0 ] <- 0
library(cluster)
topic_df_dist <-  as.matrix(daisy(t(topic_docs), metric =  "euclidean", stand = TRUE))
# Change row values to zero if less than row minimum plus row standard deviation
# keep only closely related documents and avoid a dense spagetti diagram
# that's difficult to interpret (hat-tip: http://stackoverflow.com/a/16047196/1036500)
topic_df_dist[ sweep(topic_df_dist, 1, (apply(topic_df_dist,1,min) + apply(topic_df_dist,1,sd) )) > 0 ] <- 0
plot(hclust(dist(topic.words)), labels=topics.labels)
#create data.frame
topic_docs <- data.frame(topic.docs)
names(topic_docs) <- documents$id
library(cluster)
topic_df_dist <- as.matrix(daisy(t(topic_docs), metric = "euclidean", stand = TRUE))
topic_df_dist[ sweep(topic_df_dist, 1, (apply(topic_df_dist,1,min) + apply(topic_df_dist,1,sd) )) > 0 ] <- 0
km <- kmeans(topic_df_dist, n.topics)
km <- kmeans(topic_df_dist, n.topics, replace = TRUE)
simdocs <- vector("list", length = n.topics)
for(i in 1:n.topics){
simdocs[[i]] <- names(km$cluster[km$cluster == i])
}
km <- kmeans(topic_df_dist, n.topics)
kmeans(topic_df_dist, n.topics)
km <- kmeans(topic_df_dist, n.topics, replace = FALSE)
km <- kmeans[sample(topic_df_dist, n.topics, replace = FALSE),]
km <- kmeans[sample(topic_df_dist, n.topics, replace = FALSE prob = NULL),]
km <- kmeans(topic_df_dist, n.topics, replace = FALSE prob = NULL)
km <- kmeans(topic_df_dist, n.topics)
km <- kmeans[sample(topic_df_dist), n.topics),]
km <- kmeans[sample(topic_df_dist),] n.topics)
km <- kmeans(topic_df_dist, n.topics)
km <- kmeans(topic_df_dist, n.topics, replace = TRUE )
help kmeans
km <- kmeans(topic_df_dist, n.topics, nstart = 25)
km <- kmeans(topic_df_dist, n.topics, nstart = 2)
km <- kmeans(topic_df_dist, n.topics)
km <- kmeans(topic_df_dist, n.topics)$withinss
topic_docs
names
names(topic_docs)
topic_df_dist
getwd()
setwd(~/githubstuff)
setwd("~/githubstuff")
setwd("feguson")
setwd("ferguson")
getwd()
documents <- mallet.read.dir("originaldocs\1000chunks\")
# windows users, remember: have the full path, ie "C:\\mallet-2.0.7\\sample-data\\web\\" and so on throughout this script
documents <- mallet.read.dir("originaldocs/1000chunks/")
mallet.instances <- mallet.import(documents$id, documents$text, "en.txt", token.regexp = "\\p{L}[\\p{L}\\p{P}]+\\p{L}")
n.topics <- 100
mallet.instances <- mallet.import(documents$id, documents$text, "en.txt", token.regexp = "\\p{L}[\\p{L}\\p{P}]+\\p{L}")
n.topics <- 100
topic.model <- MalletLDA(n.topics)
topic.model$loadDocuments(mallet.instances)
## Get the vocabulary, and some statistics about word frequencies.
## These may be useful in further curating the stopword list.
vocabulary <- topic.model$getVocabulary()
word.freqs <- mallet.word.freqs(topic.model)
## Optimize hyperparameters every 20 iterations,
## after 50 burn-in iterations.
topic.model$setAlphaOptimization(20, 50)
## Now train a model. Note that hyperparameter optimization is on, by default.
## We can specify the number of iterations. Here we'll use a large-ish round number.
topic.model$train(200)
documents <- mallet.read.dir("originaldocs/1000chunks/")
mallet.instances <- mallet.import(documents$id, documents$text, "en.txt", token.regexp = "\\p{L}[\\p{L}\\p{P}]+\\p{L}")
n.topics <- 100
topic.model <- MalletLDA(n.topics)
topic.model$loadDocuments(mallet.instances)
vocabulary <- topic.model$getVocabulary()
word.freqs <- mallet.word.freqs(topic.model)
## Optimize hyperparameters every 20 iterations,
## after 50 burn-in iterations.
topic.model$setAlphaOptimization(20, 50)
## Now train a model. Note that hyperparameter optimization is on, by default.
## We can specify the number of iterations. Here we'll use a large-ish round number.
topic.model$train(200)
## NEW: run through a few iterations where we pick the best topic for each token,
## rather than sampling from the posterior distribution.
topic.model$maximize(10)
#' Get the probability of topics in documents and the probability of words in topics.
#' By default, these functions return raw word counts. Here we want probabilities,
#' so we normalize, and add "smoothing" so that nothing has exactly 0 probability.
doc.topics <- mallet.doc.topics(topic.model, smoothed=T, normalized=T)
topic.words <- mallet.topic.words(topic.model, smoothed=T, normalized=T)  ##adap jockers wordcloud script to use this variable
topic.docs <- t(doc.topics)
topic.docs <- topic.docs / rowSums(topic.docs)
write.csv(topic.docs, "ferguson-topics-docs.csv" ) ## "C:\\Mallet-2.0.7\\topic-docs.csv"
topics.labels <- rep("", n.topics)
for (topic in 1:n.topics) topics.labels[topic] <- paste(mallet.top.words(topic.model, topic.words[topic,], num.top.words=5)$words, collapse=" ")
#' have a look at keywords for each topic
topics.labels
write.csv(topics.labels, "ferguson-topics-labels.csv") ## "C:\\Mallet-2.0.7\\topics-labels.csv")
#' Or we could do word clouds of the topics
library(wordcloud)
for(i in 1:10){
topic.top.words <- mallet.top.words(topic.model,
topic.words[i,], 10)
print(wordcloud(topic.top.words$words,
topic.top.words$weights,
c(4,.8), rot.per=0,
random.order=F))
}
for(i in 1:10){
topic.top.words <- mallet.top.words(topic.model,
topic.words[i,], 10)
print(wordcloud(topic.top.words$words,
topic.top.words$weights,
c(4,.8), rot.per=0,
random.order=F))
}
plot(hclust(dist(topic.words)), labels=topics.labels)
topic_docs <- data.frame(topic.docs)
names(topic_docs) <- documents$id
library(cluster)
topic_df_dist <- as.matrix(daisy(t(topic_docs), metric = "euclidean", stand = TRUE))
topic_df_dist[ sweep(topic_df_dist, 1, (apply(topic_df_dist,1,min) + apply(topic_df_dist,1,sd) )) > 0 ] <- 0
km <- kmeans(topic_df_dist, n.topics)
